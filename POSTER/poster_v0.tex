\documentclass[sanserif,mathserif,final,16pt]{beamer}
\mode<presentation>{\usetheme{SPIRE}}

\usepackage{amsmath,amsfonts,amssymb,pxfonts,xspace}
\usepackage{graphicx,multirow}
\usepackage[size=custom,width=118.9,height=84.1,scale=1.0]{beamerposter}

\usepackage[listings,theorems]{tcolorbox}
\usepackage{setspace}
\usepackage{color,xcolor}
\usepackage{booktabs,subcaption}
\usepackage{caption} % for captionof

\setbeamercolor{background canvas}{bg=burlywood!25}
\usepackage[table,xcdraw]{xcolor}

\DeclareMathOperator*{\argmin}{arg\,min}
\definecolor{darkblue}{rgb}{0.1, 0.1, 0.44}
\definecolor{lightgray}{rgb}{0.9, 0.9, 0.9}
\definecolor{burntumber}{rgb}{0.54, 0.2, 0.14}
\definecolor{sienna}{rgb}{0.53, 0.18, 0.09}
\definecolor{rosewood}{rgb}{0.4, 0.0, 0.04}
\definecolor{burlywood}{rgb}{0.87, 0.72, 0.53}
\definecolor{mahogany}{rgb}{0.75, 0.25, 0.0}

\newcommand{\footleft}{\normalsize{\hspace{0.5em}https://spire.ee.iisc.ac.in/}}
\newcommand{\footright}{\normalsize{saubkumarsk@gmail.com, sumitsharma@iisc.ac.in, prasanta@iisc.ac.in\hspace{0.5em}}}

\usepackage{pifont}
\newcommand{\starbullet}{\ding{72}}

\usepackage[utf8]{inputenc}         % Ensure UTF-8 encoding
\newcommand{\cmark}{{\color{green!60!black}\ding{51}}}     % Check mark ✔
\newcommand{\warn}{{\color{orange!80!black}\ding{46}}}     % Warning ❗
\newcommand{\xmark}{{\color{red!70!black}\ding{55}}}       % Cross ✘
\newcommand{\info}{{\color{blue!70!black}\ding{72}}}       % Info ℹ

% -------------------------------------------------------------------
% TITLE / AUTHORS / AFFILIATIONS  (exactly as camera-ready)
% -------------------------------------------------------------------

\title{\Huge\bfseries
MADASR 2.0: Multi-Lingual Multi-Dialect ASR Challenge in 8 Indian Languages
}

\author{\large
Saurabh Kumar\textsuperscript{1},
Sumit Sharma\textsuperscript{1},
Deekshitha G\textsuperscript{1},
Abhayjeet Singh\textsuperscript{1},
Amartyaveer\textsuperscript{1},
Sathvik Udupa\textsuperscript{1},
Sandhya Badiger\textsuperscript{1},\\[0.2em]
Sanjeev Khudanpur\textsuperscript{2},
Sunayana Sitaram\textsuperscript{3},
S. Umesh\textsuperscript{4},
Bhuvana Ramabhadran\textsuperscript{5},
Brian Kingsbury\textsuperscript{6},\\[0.2em]
Hema A. Murthy\textsuperscript{4},
Srikanth S. Narayanan\textsuperscript{7},
Howard Lakougna\textsuperscript{8},
Prasanta Kumar Ghosh\textsuperscript{1}
}

\institute{\small
\textsuperscript{1}Indian Institute of Science (IISc), Bengaluru, India \quad
\textsuperscript{2}Johns Hopkins University, USA \quad
\textsuperscript{3}Microsoft Research, India\\[0.2em]
\textsuperscript{4}Indian Institute of Technology Madras (IITM), India \quad
\textsuperscript{5}Google DeepMind, USA \quad
\textsuperscript{6}IBM Research, USA\\[0.2em]
\textsuperscript{7}University of Southern California (USC), USA \quad
\textsuperscript{8}Gates Foundation, USA
}

% -------------------------------------------------------------------
\begin{document}
\begin{frame}{}

\vspace{-1em}

\begin{columns}[t,totalwidth=\textwidth]

% ======================================================
% LEFT COLUMN: Intro + Baselines + Leaderboard
% ======================================================
\column{0.32\textwidth}

% ---------------- Introduction & Motivation ----------------
\begin{block}{\Large\textbf{Introduction \& Motivation}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{Why MADASR 2.0?}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item India has $100+$ spoken languages, 22 scheduled languages, and hundreds of dialects with rich phonological and lexical variation.
    \item Robust ASR for low-resource Indian languages remains difficult, even with SSL and large multilingual models.
    \item Real-world deployments must handle dialectal variation, mixed registers, and domain mismatch (read vs.\ spontaneous speech).
\end{itemize}

\vspace{0.4em}

\textbf{\textcolor{darkblue}{From MADASR 1.0 to 2.0}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item \textbf{MADASR 1.0} (ASRU 2023): monolingual dialectal ASR for Bengali \& Bhojpuri read speech.
    \item \textbf{MADASR 2.0} (ASRU 2025): \textbf{8 languages}, \textbf{33 dialects}, train on read speech, test on \textbf{read} and \textbf{spontaneous} speech.
\end{itemize}

\vspace{0.4em}

\textbf{\textcolor{darkblue}{Goals}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item Provide a \textbf{standardised benchmark} for multilingual, multi-dialect ASR in Indian languages.
    \item Study \textbf{dialect robustness} and generalisation across domains and speech styles.
    \item Encourage \textbf{multitask} and \textbf{dialect-aware} modelling with ASR, LID, and DID.
\end{itemize}

\vspace{0.4em}

\textbf{\textcolor{darkblue}{Main Contributions}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item MADASR 2.0 subset of \textbf{RESPIN}: $\sim$1,240 h of labelled read speech across 8 languages and 33 dialects.
    \item \textbf{Four tracks} spanning constrained/unconstrained settings and 30 h vs.\ 150 h per language.
    \item Open-source \textbf{ESPnet baselines}, scoring scripts, and public leaderboards for read and spontaneous speech.
\end{itemize}

\end{block}

\vspace{0.4em}

% ---------------- Baseline Systems ----------------
\begin{block}{\Large\textbf{Baseline Systems \& Observations}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{ESPnet Conformer Baselines}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item Hybrid CTC/Attention ASR in ESPnet with \textbf{Conformer} encoder and \textbf{Transformer} decoder.
    \item Tracks 1 \& 2: supervised training on 30 h and 150 h RESPIN subsets.
    \item Tracks 3 \& 4: same subsets with encoder initialised using frozen \textbf{IndicWav2Vec} SSL features.
    \item Recipes and pretrained checkpoints released on GitHub \& Hugging Face.
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Dev-Set Performance (Overall)}}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Track} & \textbf{CER} & \textbf{WER} & \textbf{LID Acc.} \\
\midrule
1 (30 h, const.)  & 4.06 & 17.28 & 97.41 \\
2 (150 h, const.) & \textbf{3.61} & \textbf{15.72} & 96.58 \\
3 (30 h + SSL)    & 4.36 & 18.45 & 96.92 \\
4 (150 h + SSL)   & 3.89 & 16.74 & 96.18 \\
\bottomrule
\end{tabular}

\vspace{0.4em}
\raggedright
\textbf{\textcolor{darkblue}{Insights}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item \textbf{Scale helps:} Track 2 (150 h) achieves the lowest CER/WER.
    \item Frozen SSL features are competitive but not consistently better than fully supervised training.
    \item Kannada and Telugu show relatively higher WER, especially on spontaneous speech.
\end{itemize}

\end{block}

\vspace{0.4em}

% ---------------- Leaderboard Highlights ----------------
\begin{block}{\Large\textbf{Leaderboard Highlights}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{Read Speech (Test-read)}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item Best CERs range from \textbf{4.30–4.98\%} across tracks.
    \item \textbf{Track 2} (150 h, constrained) achieves the strongest ASR: baseline and SPRING Lab around \textbf{4.3–4.4\%} CER.
    \item SPRING Lab attains top DID accuracy (up to \textbf{75.4\%}) while maintaining low CER.
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Spontaneous Speech (Test-spontaneous)}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item CERs rise sharply to \textbf{23–68\%}, indicating severe domain mismatch.
    \item Pramiteeh and baseline systems lead with CER $\approx$\textbf{24–26\%} across tracks.
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Language-wise Behaviour}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item Bhojpuri, Bengali, and Maithili generally achieve lower CER.
    \item Kannada, Magahi, and Telugu show higher CER, especially for spontaneous speech.
\end{itemize}

\end{block}

% ======================================================
% MIDDLE COLUMN: Dataset + Portal + Key Takeaways
% ======================================================
\column{0.34\textwidth}

% ---------------- Dataset Block ----------------
\begin{block}{\Large\textbf{Dataset: RESPIN Subset for MADASR 2.0}}
\vspace{0.3em}

\begin{columns}[t]
\begin{column}{0.56\textwidth}
\centering
\includegraphics[width=\linewidth]{presentation/Images/madasr_lang_map.pdf}
\vspace{0.4em}

{\small
Geographic coverage of RESPIN languages and district-level dialects.
}
\end{column}
\begin{column}{0.42\textwidth}
\small
\textbf{\textcolor{darkblue}{Languages (8):}}
\begin{itemize}\setlength\itemsep{0.15em}
    \item Bengali (bn), Bhojpuri (bh)
    \item Chhattisgarhi (ch), Kannada (kn)
    \item Magahi (mg), Maithili (mt)
    \item Marathi (mr), Telugu (te)
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Dialect Labels}}
\begin{itemize}\setlength\itemsep{0.15em}
    \item Fine-grained \textbf{district-level} DID tags.
    \item Total: \textbf{33 dialects} across 8 languages.
\end{itemize}
\end{column}
\end{columns}

\vspace{0.4em}
\small
\textbf{\textcolor{darkblue}{Subsets and Scale}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item \textbf{Train-small} (Tracks 1 \& 3): $\approx$30 h / language, \textbf{241.1 h}, 146{,}760 utts, 9{,}387 spkrs.
    \item \textbf{Train-large} (Tracks 2 \& 4): $\approx$150 h / language, \textbf{1{,}239.5 h}, 733{,}800 utts, 11{,}315 spkrs.
    \item \textbf{Dev (read):} 17.6 h, 11{,}507 utts, 660 spkrs.
    \item \textbf{Test-read:} 26.7 h, 17{,}550 utts, 1{,}320 spkrs.
    \item \textbf{Test-spontaneous:} 7.0 h, 5{,}400 utts, 5{,}400 spkrs.
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Annotation}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Each utterance: LID, DID, speaker ID, transcript, duration.
    \item Transcripts in native scripts; numbers written out, English transliterated.
    \item No further text normalisation $\Rightarrow$ orthographic dialectal variation preserved.
\end{itemize}

\end{block}

\vspace{0.4em}

% ---------------- Portal & Evaluation ----------------
\begin{block}{\Large\textbf{Challenge Setup, Portal \& Evaluation}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{Web Portal}}
\begin{itemize}\setlength\itemsep{0.25em}
    \item React-based portal with team registration and password-protected submission.
    \item Backend API parses TSVs, runs scoring scripts, and updates public leaderboards.
\end{itemize}

\vspace{0.4em}
\textbf{\textcolor{darkblue}{Timeline (AoE)}}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
Registration opens        & April 10, 2025 \\
Train + Dev release       & April 19, 2025 \\
Baseline systems release  & April 22, 2025 \\
Test set release          & May 25, 2025 \\
Submission portal opens   & May 31, 2025 \\
Final submission deadline & June 21, 2025 \\
Leaderboard results       & June 22, 2025 \\
Challenge paper deadline  & June 25, 2025 \\
\bottomrule
\end{tabular}

\vspace{0.4em}
\raggedright
\textbf{\textcolor{darkblue}{Scoring Protocol}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item \textbf{ASR:} CER and WER via \texttt{jiwer}. LID/DID tokens stripped; a small set of invalid utts excluded.
    \item Punctuation removed from hypotheses; no further normalisation.
    \item \textbf{LID/DID:} utterance-level accuracy; exact token match required.
\end{itemize}

\end{block}

\vspace{0.4em}

% ---------------- Key Takeaways ----------------
\begin{block}{\Large\textbf{Key Takeaways}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{1. In-domain Supervision is Crucial}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Track 2 (150 h, constrained) consistently outperforms Track 1 (30 h) in CER/WER.
\end{itemize}

\vspace{0.2em}
\textbf{\textcolor{darkblue}{2. SSL Features Need Careful Use}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Frozen IndicWav2Vec features (Tracks 3 \& 4) are competitive but not consistently superior to supervised training.
\end{itemize}

\vspace{0.2em}
\textbf{\textcolor{darkblue}{3. Domain Mismatch is Hard}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Spontaneous speech is much harder than read speech for all systems, highlighting the need for domain adaptation.
\end{itemize}

\vspace{0.2em}
\textbf{\textcolor{darkblue}{4. Multitask \& Dialect-aware Modelling Helps}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Multitask models (ASR + LID + DID + demographics) can improve auxiliary tasks with modest ASR degradation.
\end{itemize}

\vspace{0.2em}
\textbf{\textcolor{darkblue}{5. MADASR 2.0 as a Benchmark}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Provides a public, dialect-rich benchmark with standardised protocols and strong baselines for Indian-language ASR.
\end{itemize}

\end{block}

% ======================================================
% RIGHT COLUMN: Tracks/Tasks + Participation + Conclusion/Links
% ======================================================
\column{0.32\textwidth}

% ---------------- Challenge Design ----------------
\begin{block}{\Large\textbf{Challenge Design: Tracks \& Tasks}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{Tracks}}
\centering
\small
\begin{tabular}{@{}llcc@{}}
\toprule
\textbf{Track} & \textbf{Setting} & \textbf{Train} & \textbf{External} \\
\midrule
1 & Constrained, low  & 30 h/lang  & No  \\
2 & Constrained, high & 150 h/lang & No  \\
3 & Unconstr., low    & 30 h/lang  & Yes \\
4 & Unconstr., high   & 150 h/lang & Yes \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\raggedright
\textbf{\textcolor{darkblue}{Primary Task: ASR}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Multilingual, multidialect ASR across 8 languages.
    \item Evaluation on \textbf{read} and \textbf{spontaneous} test sets.
    \item Metrics: \textbf{CER} and \textbf{WER} (using \texttt{jiwer}).
\end{itemize}

\vspace{0.2em}
\textbf{\textcolor{darkblue}{Auxiliary Tasks}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item \textbf{LID:} language ID (e.g., [bn]).
    \item \textbf{DID:} dialect ID (e.g., [bn\_D3]).
    \item Metric: utterance-level accuracy.
\end{itemize}

\vspace{0.2em}
\textbf{\textcolor{darkblue}{Submission Format (TSV)}}
\small
\begin{itemize}\setlength\itemsep{0.15em}
    \item \textbf{ASR only:} \texttt{utt\_id \quad transcription}
    \item \textbf{ASR + LID:} \texttt{utt\_id \quad transcription \quad [bh]}
    \item \textbf{ASR + LID + DID:} \texttt{utt\_id \quad transcription \quad [bh D1]}
\end{itemize}

\end{block}

\vspace{0.4em}

% ---------------- Participation & Systems ----------------
\begin{block}{\Large\textbf{Participation \& Submitted Systems}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{Participation}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item \textbf{171} dataset downloads from \textbf{28} countries.
    \item \textbf{80} registered teams from \textbf{8} countries.
    \item \textbf{306} submissions; \textbf{191} valid for ranking.
    \item \textbf{7} teams submitted at least one valid final system.
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Representative Approaches}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item \textbf{Team YS:} fine-tuned \textbf{Whisper-large-v3} with \textbf{MixLoRA} (LoRA-based mixture-of-experts).
    \item \textbf{SPRING Lab:} Common Label Set (CLS) via G2P-based mapping and dual decoders (CLS \& native script).
    \item \textbf{QWER:} voice conversion, mixed-speech augmentation, LID tokens, and error-aware fine-tuning (upsampling high-error dialects).
    \item \textbf{Pramiteeh:} multitask model for ASR, LID, DID, gender, and age with shared encoder and language-specific ASR decoders.
\end{itemize}

\end{block}

\vspace{0.4em}

% ---------------- Conclusion ----------------
\begin{block}{\Large\textbf{Conclusion \& Future Directions}}
\vspace{0.3em}

\textbf{\textcolor{darkblue}{Summary}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item MADASR 2.0 evaluates multilingual, multidialect ASR across 8 Indian languages and 33 dialects using RESPIN.
    \item Four tracks, two test conditions (read \& spontaneous), and optional LID/DID enable systematic study of robustness and generalisation.
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Future Work}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Better modelling of spontaneous conversational speech, including disfluencies and code-switching.
    \item End-to-end dialect-aware models that jointly handle ASR, LID, DID, and domain adaptation.
    \item Parameter-efficient adaptation schemes for dialect and style shifts.
\end{itemize}

\vspace{0.3em}
\textbf{\textcolor{darkblue}{Acknowledgements}}
\begin{itemize}\setlength\itemsep{0.2em}
    \item Supported by the \textbf{RESPIN} project, funded by the Gates Foundation.
    \item We thank the RESPIN team, Navana Tech, and all MADASR 2.0 participants.
\end{itemize}

\end{block}

\vspace{0.3em}

% ---------------- Links & References ----------------
\begin{block}{\footnotesize\textbf{Links \& References}}
\begin{columns}[t]
\begin{column}{0.60\textwidth}
\scriptsize
\textbf{\textcolor{darkblue}{Key Links}}
\begin{itemize}\setlength\itemsep{0.15em}
    \item Challenge: \texttt{https://sites.google.com/view/respinasrchallenge2025}
    \item RESPIN corpus: \texttt{https://respin.iisc.ac.in}
    \item Baseline ESPnet recipes and models: GitHub \& Hugging Face
\end{itemize}

\vspace{0.2em}
\textbf{\textcolor{darkblue}{Selected References}}
\begin{itemize}\setlength\itemsep{0.15em}
    \item Baevski et al., wav2vec 2.0, NeurIPS 2020.
    \item Radford et al., Whisper, 2022.
    \item Diwan et al., MUCS 2021, Interspeech 2021.
    \item Amartyaveer et al., ICASSP 2025 (RESPIN DID).
    \item Kumar et al., Interspeech 2025 (ASR–DID fusion).
    \item RESPIN corpus \& MADASR 2.0 challenge website (camera-ready paper).
\end{itemize}
\end{column}
\begin{column}{0.40\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{presentation/Images/madasr_qrcode.pdf}\\[0.3em]
{\scriptsize (Challenge website / paper)}
\end{column}
\end{columns}
\end{block}

\end{columns}

\end{frame}
\end{document}